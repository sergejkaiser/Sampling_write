\section{Methodology}

\subsection{What are missing data in surveys?}

Survey data may contain different kinds of non response. First unit non response describes the situation if there is no data of the target unit (mostly a person) or item non response describes that a sampling unit did not respond to a particular item. \par
%Explanation that there are traditional approaches to missing data techniques and modern more general approaches. 
% In general t
<<<<<<< HEAD
Schaffer and Graham (2002) describe that traditionally for the first kind survey statisticians used reweighting whereas for the latter they used single imputation.
Further the authors note that this methods may provide in special cases similar performance as modern methods like maximum likelihood based methods or multiple imputation methods, which are more general. \par
=======
Schafer and Graham (2002) describe that traditionally for the first kind survey statistician used reweighing whereas for the latter they used single imputation.
 Further the author note that this methods may provide in special cases similar performance as modern methods like maximum likelihood based methods or multiple imputation methods, which are more general.  \par
>>>>>>> refs/remotes/origin/master
In the following we will first outline a missing data classification based on Rubin (1976). 
Next, based on the classification we explain why we need missing data techniques. 
The discussion of missing data techniques follows then. In particular, we will discuss traditional methods and compare them to more advanced methods. 
Finally, we discuss modern missing data methods. \par

\subsection{Which kind of missing data exist?} 
Since Rubin (1976) missing data is analyzed with probability models. 
To illustrate we will discuss the case of an arbitrary pattern of nonresponse.
In a survey of $Y$ questions, we may or may not observe a nonresponse to a question from each participant. 
We will record for each question the non response with a binary variable $R_Y$. 
The variable value is equal to zero, if the survey participant answered a particular question and is one if he did not answer. 
For an arbitrary non response pattern our survey data set, where each of the $Y$ questions corresponds to a separate column and each of the $n$ survey participants corresponds to a row, we thus obtain a set of binary indicators $R$.
The innovation of Rubin (1976) is to study the missingness mechanism Rubin and Little (2002).The missingness mechanism describes the relationship between the missingness and the variables in the data set of variables. Rubin (1976) showed how to study the mechansim by treating $R$ as a random variable with a distribution. \par
Based on the missing data mechanism we can classify non response into three categories missing completely at random (MCAR), missing at random(MAR), not missing at random (MNAR), we discuss each category in turn.  Missingness completely at random describes that the probability that a value is missing is independent of our missing or observed data and the missing data mechanism. 
Missing at random describes that the probability of a missing value does only depend on our observed data but not on the missing values. 
Last the assumption not missing at random describes that missingness depends on observed and unobserved parts of the data. \par

\subsection{Why are missing data techniques necessary?}
<<<<<<< HEAD

The concerns about non response are (1) efficiency losses (2) complications in data handling and data analysis (3) bias due to differences between data values for those between respondents and non respondents  Schafer (1998). \par 
Point (1) and (3) can be ignored if the missing data is MCAR. 
=======
The concerns about non response are (1) efficiency losses (2) complications in data handling and data analysis (3) bias due to differences between data values for those between respondents and non respondents  Schafer (1999). \par 
Point (1) and (3)  can be ignored if the missing data is MCAR. 
>>>>>>> refs/remotes/origin/master
Therefore the missingness in our survey would be independent to all other collected survey variables. MAR is in practice unlikely to hold. \par 
Usually the missingness is related to other variables in the data set and hence either MAR or MNAR. 
If we assume that the missingness in our survey is  MAR we assume that we can correct for the missingness in our data set with the rest of our data Van den Beuren (?).  Most of the modern missing data are based on this assumption. Further, if the missingness is MNAR the missingness mechanism has to be taken into account in the analysis. An example would be if respondents would self select into the survey based on unobserved characteristics. 

\subsection{Traditional Missing Data techniques} 

\begin{itemize}
\item Listwise Complete Observations
This method keeps only the responses from the survey participants which completely answered the questions which are analysed. E.g. if we build a regression model for sentiment towards immigrants this method keeps only the observations, which have completed all questions included as independent or dependent variables.
In general this method may be used if the missingness is MCAR. \par However in the case of multivariate analysis for a large number of variables, even small fractions of missing values for each variable can lead to a large reduction of the sample size with this method. Therefore the authors note that even under MCAR listewise complete observations may be not efficient. \par Under certain patterns of MAR the method yields valid and efficient estimates of the regression coefficients. This does not extend to correlation. In general if the fraction of missing values is not to large, this method will not lead to much bias (Schafer and Graham 2002).  
\item Pairwise Complete Observations 
<<<<<<< HEAD
This method keeps different sets of sample units for different parameters. To highlight this we look at the same example as before. Further, we refine the example by specifying that regression model studies the relation between $Y_{1,i}$ and $X_{1,i}$ and $X_{2,i}$ for all sampling units $i$, which where obtained by simple random sampling. Under pairwise complete observations the parameter $\beta_1$, which describes the relation between  $Y_{1,i}$ and $X_{1,i}$, would be obtained using all available observations. The same holds for the parameter $\beta_2$. For each pair $Y_{1,i} , X_{1,i}$  and  $Y_{1,i} , X_{2,i}$ the pairwise complete observations would be used, therefore the sets of observations would be different for each parameter. According to Schaffer and and Graham (2002) the computation of the standard errors thus becomes difficult. Further, they showed a problem of this method with the example of a correlation. Eg. to calculate the correlation between $Y_{1,i} , X_{1,i}$, one may use all the available values of $X_{1,i}$ to calculate the standard deviation of $X_{1,i}$ and all available pairs of $Y_{1,i}, X_{1,i}$ to obtain the covariance. However, a correlation obtained like this may be not bound in the interval $[-1,1]$. Therefore the authors conclude that although the principle to use as much data as available may be good, the particular implementation is not good.
=======
This method keeps different sets of sample units for different parameters. To highlight this we look at the same example as before. Further, we refine the example by specifying that regression model studies the relation between $Y_{1,i}$ and $X_{1,i}$ and $X_{2,i}$ for all sampling units $i$, which where obtained by simple random sampling. Under pairwise complete observations the parameter $\beta_1$, which describes the relation between  $Y_{1,i}$ and $X_{1,i}$, would be obtained using all available observations. The same holds for the parameter $\beta_2$. For each pair $Y_{1,i} , X_{1,i}$  and  $Y_{1,i} , X_{2,i}$ the pairwise complete observations would be used, therefore the sets of observations would be different for each parameter. According to Schafer and and Graham (2002) the computation of the standard errors thus becomes difficult. Further,  they showed a problem of this method with the example of a correlation. Eg. to calculate the correlation between $Y_{1,i} , X_{1,i}$, one may use all the available values of $X_{1,i}$ to calculate the standard deviation of $X_{1,i}$ and all available pairs of $Y_{1,i} , X_{1,i}$ to obtain the covariance. However, a correlation obtained like this may be not bound in the interval $[-1,1]$. Therefore the authors conclude that although the principle to use as much data as available may be good, the particular implementation is not good.
>>>>>>> refs/remotes/origin/master
 \item Weighting
The method is used in combination with listwise complete observations. Schafer and Graham (2002) note that under specific conditions weighting can reduce the bias of the method in the case of MAR and NMAR. As in listwise complete observation the sample is reduced to the set of complete observations for a particular analysis. The sample is in the next step adjusted to resemble more closely the population or the full sample with regards to covariates. The weights are based on estimates of the probability to respond, which may be obtained from the data. The weighting eliminates the bias due to non response for the included variables in the probability model. However, the bias for any variables not included in the model will not be reduced.
\end{itemize}
 \begin{itemize} 
\item Single imputation
\begin{itemize}
\item Mean substitution
Imputing a missing value due to item non response with the mean value of the corresponding question yields unbiased estimates. However, we do not account for the uncertainty introduced by the missing value. The method downward biases the sample variance and  overstates the number of observations. Further Schafer and and Graham (2002) report that this method biases the covariance between variables and the interclasscorrelation. 
\item  Regression imputation
This statistical technique imputes the missing values with the predicted values from a regression. The method produces unbiased estimates of the mean under MCAR and MAR however the imputed data will show less variation than the complete data (Enders and Baraldi 2010). The problem can be mitigated by adding a random component to each predicted score, which is drawn from a normal distribution with mean zero and a variance equal to the residual variance. This method yields unbiased estimates (both under MCAR and MAR) , however the standard errors do not account for the uncertainty of the estimates. The result is that the standard error is downward biased and statistical test will have a higher type 1 error. 
 \end{itemize}  

\end{itemize}  
\subsection{Modern Techniques} 
 In the following we describe \textbf{multiple imputation} based on the outline in Schafer (1999). \par  
Multiple imputation is a simulation based technique. Each missing value is imputed with $m$  simulated  values, where $m$ is usually a small number (3-10). The method leads to valid imputation under a frequentist perspective under the assumption of MAR and an additional technical assumption about the parameters of the missigness mechanism and the analysis model. \par 
Rubin and Little (1987) recommend to implement MI with bayesian techniques. Therefore the simulated values are draws from the predictive posterior distribution, which is obtained under bayesian estimation.  \par
 With this technique a specific missing data model can be specified, which can be different from the analysis model. The choice of the imputation model is not completely free and must be done with attention to the analysis model. A guidance is that the posterior distribution of the imputation model must reflect uncertainty about the analysis model. \par 
After the imputation  the data user to analyze each of the $m$ imputed data sets using complete data techniques. Finally, the $m$ results can be combined using Rubin's Rules (Rubin 1987).  Rubin showed that an estimate can be pooled using an average of the $m$ results. Further the estimated total variance is a combination of the average within imputation variance and the between imputation variance. Statistical tests about the estimate may be conducted using a student t-distribution with a MI specific degrees of freedom (see Schafer 1999).   \par The flexibility of this approach is due to the seperation of the imputation and the analysis step. In general as Schafer (1999) notes that the imputation need to be reasonable and reflect uncertainty. As the analysis and imputation are separated it may be the case that the assumptions of the imputation model and the analysis model are incompatible. Schafer (1999) notes that if the imputation model makes less assumptions than the  analysis model than the MI estimate is valid however a loss of power may occur. Further, in case the imputation model makes more assumptions and these assumptions are valid the MI estimate may be more efficient than the complete data analysis. \par 
In the following we describe the use of \textbf{maximum likelihood methods for estimation with missing data} based on Enders (2006). 
Under the MAR assumption maximum likelihood methods may be used to obtain valid estimates. \par
<<<<<<< HEAD
 The estimation of missing data maximum likelihood requires the maximization of the log likelihood function, which is the log of the likelihood function. The likelihood function describes how likely a particular parameter estimate is given the data. The ML estimation yields the most likely parameter estimate minimizing the squared distance between the parameter and the data. \par With missing data present the maximum likelihood estimation changes only little. The estimation uses for each individual all available data to estimate the most likely parameter given the data.  
=======
 The estimation of missing data maximum likelihood requires the maximization of the log likelihood function, which is the log of the likelihood function. The likelihood function describes how likely a particular parameter estimate is given the data. The ML estimation yields the most likely parameter estimate minimizing the squared distance between the parameter and the data. \par With missing data present the maximum likelihood estimation changes only little. The estimation uses for each individual all available data to estimate the most likely parameter given the data.
% @book{rubin1987multiple,
%  title={Multiple imputation for nonresponse in surveys},
%  author={Rubin, Donald B},
%  year={1987},
%  publisher={John Wiley \& Sons, Inc.}
%}
 %@book{Little:1986:SAM:21412,
% author = {Little, Roderick J A and Rubin, Donald B},
% title = {Statistical Analysis with Missing Data},
% year = {1986},
% isbn = {0-471-80254-9},
% publisher = {John Wiley \& Sons, Inc.},
% address = {New York, NY, USA},
%} 
 %%@article{rubin1976
%  title={Inference and missing data},
%  author={Rubin, Donald B},
%  journal={Biometrika},
%  volume={63},
%  number={3},
%  pages={581--592},
%  year={1976},
%  publisher={Biometrika Trust}
%}
 %@article{schafer2002missing,
%  title={Missing data: our view of the state of the art.},
%  author={Schafer, Joseph L and Graham, John W},
%  journal={Psychological methods},
%  volume={7},
%  number={2},
%  pages={147},
%  year={2002},
%  publisher={American Psychological Association}
%}
  %@article{baraldi2010introduction,
 % title={An introduction to modern missing data analyses},
 % author={Baraldi, Amanda N and Enders, Craig K},
 % journal={Journal of School Psychology},
 % volume={48},
 % number={1},
 % pages={5--37},
 % year={2010},
 % publisher={Elsevier}
%}

%@article{Enders06,
%	Author = {Enders, Craig K},
%	Date-Added = {2016-06-20 20:35:10 +0000},
%	Journal = {Psychosomatic medicine},
	%Pages = {427--436},
	%Title = {A primer on the use of modern missing-data methods in psychosomatic medicine research.},
%	Volume = {68},
%	Year = {2006},
%Bdsk-Url-1 = {http://dx.doi.org/10.1097/01.psy.0000221275.75056.d8}}

\endinput
>>>>>>> refs/remotes/origin/master
